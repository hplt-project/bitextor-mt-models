####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: opustrainer_final_tiny
  src: gl
  trg: en
  src_three_letter: glg
  trg_three_letter: eng
  forward-model: "/scratch/project_2005815/OpusTrainer/models/gl-en_big"
  vocab: "/scratch/project_2005815/OpusTrainer/models/gl-en_big/vocab.spm"
  
  #OPUS models are not ensembled, they have different vocabs anyway
  teacher-ensemble: 1

  #URL to the OPUS-MT model to use as the teacher
  opusmt-teacher: ""
  #URL to the OPUS-MT model to use as the backward model
  opusmt-backward: "https://object.pouta.csc.fi/Tatoeba-MT-models/eng-roa/opus-2020-06-28.zip"

  # path to a pretrained backward model (optional)  
  backward-model: ""
  
  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000

  # split corpus to parallelize translation
  split-length: 2000000
  
  best-model: perplexity
  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      tc_Tatoeba-train-v2021-08-07.swa.eng: 0.5

#marian-args:
#  decoding-teacher:
    # 2080ti or newer
#    precision: float16

#TODO: extract this info straight from the OPUS model yml info file
datasets:
  # parallel training corpus
  train:
    - tc_Tatoeba-Challenge-v2021-08-07
    - custom-corpus/cluvi
  # datasets to merge for validation while training. Translations by teacher
  devtest:
   - custom-corpus/flores200-dev
   - custom-corpus/flores200-devtest
  # datasets for evaluation
  test:
    - flores_dev
    - flores_devtest  
