####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###


experiment:
  name: opusmt
  src: ca
  trg: en
  src_three_letter: cat
  trg_three_letter: eng

  #OPUS models are not ensembled, they have different vocabs anyway
  teacher-ensemble: 1

  #URL to the OPUS-MT model to use as the teacher
  opusmt-teacher: "https://object.pouta.csc.fi/Tatoeba-MT-models/cat-eng/opus+bt-2021-04-30.zip"

  #URL to the OPUS-MT model to use as the backward model
  opusmt-backward: "https://object.pouta.csc.fi/Tatoeba-MT-models/eng-cat/opus+bt-2021-04-10.zip"

  #Only specify this if model is target-multilingual
  target-language-token: 

  # path to a pretrained backward model (optional)  
  backward-model: ""
  
  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000

  spm-sample-size: 2000000

  # split corpus to parallelize translation
  split-length: 2000000
  
  best-model: perplexity
  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      tc_Tatoeba-train-v2021-08-07.cat.eng: 0.5

#TODO: extract this info straight from the OPUS model yml info file
datasets:
  # parallel training corpus
  train:
    - tc_Tatoeba-Challenge-v2021-08-07
  # datasets to merge for validation while training. Translations by teacher
  devtest:
    - flores_dev
  # datasets for evaluation
  test:
    - flores_devtest  
  mono-src:
    - custom-corpus/catext
    - custom-corpus/macocu
    
marian-args:
  decoding-teacher: #added to reduce time
    mini-batch: 64
    beam-size: 4 
